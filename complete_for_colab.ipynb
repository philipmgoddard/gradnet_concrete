{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture optimisation using machine learning\n",
    "\n",
    "In this tutorial we will be investigating how to apply machine learning techniques to a mixture optimisation problem.\n",
    "\n",
    "This is a important step in a 'designed experiment', where the exact values of a mixture is determined using a strategic methodology. Such mixture design experiments are often used in the fields of pharmaceutical and material design.\n",
    "\n",
    "We will be following a study by Yeh (Journal of Materials in Civil Engineering, 18, 597â€“604), which is explored by Kuhn and Johnson in their book, Applied Predictive Modelling.\n",
    "\n",
    "Concrete is an integral part of most industrialized societies. It is used to some extent in nearly all structures and in many roads. One of the main properties of interest (beside cost) is the compressive strength of the hardened concrete.\n",
    "\n",
    "We will be using a data set which provides us with several hundred concrete mixtures and their compressive strength. Our overall aim is to build a model which can accurately predict compressive strength based on the ingredients, and then to use this model to predict the strongest mixtures, which can lead further experiment.\n",
    "\n",
    "We will be making use of several scientific Python libraries to follow through elements of this experiment. Specifically, will be using Pandas to hold and manipulate structured data, Matplotlib to visualise our data, Scikit-Learn to build machine learning models, and Scipy to perform an optimisation.\n",
    "\n",
    "\n",
    "Our objectives are:\n",
    "\n",
    "1) Introduce Pandas, and to use the library to hold our data in a structured format\n",
    "\n",
    "2) Use Matplotlib to visualise our data, and gain some intuition of what ingredients and relationships may be important for our model\n",
    "\n",
    "3) Introduce Scikit-Learn, and use it to build some regression models so that we can accurately predict compressive strenght from the mixture\n",
    "\n",
    "4) Perform an optimisation process using SciPy to create some predictions of the optimal concrete mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: introducing Pandas\n",
    "\n",
    "Those of you who have never used jupyter notebooks before, we have an interactive python session with inline documentation using markdown.\n",
    "\n",
    "We will start by loading in our raw data using a library called pandas.\n",
    "\n",
    "Pandas (short for panel data) gives us access to a dataframe, which is an abstraction allowing us to hold data in a more convenient object than a list or array. The dataframe is also used in languages like R and frameworks like Spark to make processing data easy. Think of it like a spreadsheet on steroids, with a huge amount of built in functionality for manipulating our data. \n",
    "\n",
    "Pandas is an open-sourced library. There is a huge community of data scientists and analysts who use it and contribute to it, meaning it is high quality, under continuous development, and any bugs are fixed very rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we load a library into our namespace like so\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas can easily read in data from a number of sources\n",
    "url = \"https://raw.githubusercontent.com/philipmgoddard/gradnet_concrete/master/raw/concrete.csv\"\n",
    "concrete_raw = pd.read_csv(url, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also get a summary of our data with the info() method\n",
    "concrete_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas allows us to answer practically any question we might have about the data. The API docs can be found [here]( http://pandas.pydata.org/pandas-docs/stable/api.html).\n",
    "\n",
    "The library is huge, so unless it is all you use every day it is hard to memorise. From experience, you can begin to  understand what type of manipulations and summaries are possible, and then you can easily consult the API docs or Stack Overflow to solve your problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets have a little play with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how to we select data?\n",
    "\n",
    "# all rows, one column\n",
    "# note a 'series' is returned here\n",
    "concrete_raw.loc[:, 'CompressiveStrength'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the first three rows\n",
    "concrete_raw.loc[0:3, 'CompressiveStrength'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pass in a list to access multiple columns (note a dataframe is returned)\n",
    "concrete_raw.loc[0:3, ['Water', 'CompressiveStrength']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can apply summary functions to columns\n",
    "concrete_raw.loc[:, 'CompressiveStrength'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can peform groupings and aggregation\n",
    "# what is the average compressive strength for different ages of concrete?\n",
    "(\n",
    "    concrete_raw\n",
    "        .groupby('Age')['CompressiveStrength']\n",
    "        .agg(['mean', 'count'])\n",
    "        .reset_index()\n",
    "        .sort_values(by='Age')   \n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can query by filtering rows based on conditions\n",
    "concrete_raw.loc[concrete_raw['Age'] == 28, :].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or a different syntax\n",
    "concrete_raw.query('Age == 28').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can create new columns\n",
    "concrete_raw.assign(cStrengthSq = lambda x: x.CompressiveStrength ** 2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas is really useful!\n",
    "\n",
    "It has a huge set of functionality. Unfortunately it can be a bit overwhelming at first, and requires a bit of a learning curve to get used to it, and there are often multiple ways to accomplish the same task.\n",
    "\n",
    "If you deal with a lot of data, it is far more flexible and resilient than Excel or other spreadsheet programs.\n",
    "\n",
    "Practice is by far the best way to get to grips with it, so why not try it for some of your work? You can write scripts which run end to end to take your data from raw to processed, which means you have fully reproducible work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do some useful work on our data now\n",
    "\n",
    "### 1) Is there any missing data? \n",
    "\n",
    "We determine which rows are null using [pd.isnull](http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.isnull.html)\n",
    "\n",
    "We  can get a series of true or false for each row, depending if they contain nulls, like this: concrete_raw.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets use this result to filter the rows in our dataframe that contain missing data\n",
    "concrete_raw.loc[concrete_raw.isnull().any(axis = 1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Are there any duplicates inputs? How might we deal with these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, lets create a list of the feature names to easily select them\n",
    "\n",
    "outcome_name = 'CompressiveStrength'\n",
    "\n",
    "feature_names = concrete_raw.columns.values.tolist()\n",
    "feature_names.remove(outcome_name)\n",
    "\n",
    "print(feature_names)\n",
    "print(outcome_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the duplicated method will give a series of true or false if the row is duplicated anywhere\n",
    "# we can sum this boolean matrix (true = 1, false = 0) to let us know how many rows are duplicates\n",
    "(\n",
    "    concrete_raw.loc[:, feature_names]\n",
    "        .duplicated()\n",
    "        .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets take a strategy to resolve this:\n",
    "# where we see multiple measurements for the same mixture,\n",
    "# we will average the observations\n",
    "\n",
    "concrete = (\n",
    "   concrete_raw\n",
    "        .groupby(feature_names)\n",
    "        .agg('mean')\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "print(\"old dimensions: {} rows, {} columns\".format(concrete_raw.shape[0], concrete_raw.shape[1]))\n",
    "print(\"new dimensions: {} rows, {} columns\".format(concrete.shape[0], concrete.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Exploring your data\n",
    "\n",
    "Before we even think about modelling, we need to understand what we are dealing with. As we have limited time, we will context some exploration using visualisations.\n",
    "\n",
    "We will use a library called matplotlib, which is another open sourced project. It is a general purpose plotting library, inspired by matlab.\n",
    "\n",
    "For a regression task, we want to be able to predict out outcome (Compressive Strength) based on our inputs. Lets try and see if there are any obvious relations between the features and the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in matplotlib and numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# cell 'magic' which allows us to render our plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define two numpy arrays for our x and y data\n",
    "x = np.arange(10)\n",
    "y = x * 3\n",
    "\n",
    "print(\"x, y pairs are: {}\".format(list(zip(x, y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, color = \"red\", label = \"i am a label\")\n",
    "plt.xlabel(\"hat\")\n",
    "plt.ylabel(\"cat\")\n",
    "plt.legend()\n",
    "plt.title(\"I am a title\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, matplotlib contains a huge functionality, and the [API documentation](https://matplotlib.org/) is the best place to get started. Also consider these [SciPy lecture notes](https://www.scipy-lectures.org/).\n",
    "\n",
    "We will use matplotlib to explore our data. Matplotlib works natively with numpy arrays of data, but has in built flexibility to work with pandas as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets visualise cement vs compressive strength\n",
    "# alpha controls the transparency\n",
    "plt.scatter(x = concrete.loc[:, \"Cement\"], y = concrete.loc[:, \"CompressiveStrength\"], alpha = 0.6)\n",
    "plt.xlabel(\"Cement\")\n",
    "plt.ylabel(\"Compressive Strength\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about the relationship between the amount of cement in the mixture and compressive strength?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Explore the relationships between the features and the outcome\n",
    "\n",
    "Discuss with your neighbor which features you think may be important to predict compressive strength based on the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- What can we say about the relationship between the features and the outcome? Do we see any non-linear relationships?\n",
    "\n",
    "- Do all features look useful\n",
    "\n",
    "- Do some mixtures lack ingredients?\n",
    "\n",
    "You may wonder how you can model the compressive strength of the mixture based on the ingredients using such messy looking data. In the next section, we will use a powerful machine learning algorithm that can combine the features to accurately predict the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Building regression models\n",
    "\n",
    "In this section, we will use our data to build a regression model to predict the compressive strength of concrete based on the mixture.\n",
    "\n",
    "We will be using the Scikit-Learn library, the de-facto python library for building machine learning models. It has a clean, consistent API for machine learning algorithms, and various capabilities for data preparation. More advanced users may build machine learning 'pipelines' which pass data through various transformation steps to process it finally making predictions with a model.\n",
    "\n",
    "Building machine learning algorithms effectively can require a lot of experience; as well as model performance there are many considerations that need to be taken into account, such as how your model will be used once it is built.\n",
    "\n",
    "If you are interested in building experience in this area, there are a number of sources of data sets, such as UC Irvine ML repository, various high-quality online courses (Coursera, EdX, Udemy etc), and some good blogs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by using a model called a random forest.\n",
    "\n",
    "This model builds a number of decision trees, where each split aims to separate the data into two groups which minimise the sum of squares error:\n",
    "\n",
    "$$ SSE = \\sum_{i \\in S_1} (y_i - \\bar{y}_1)^2 + \\sum_{i \\in S_2} (y_i - \\bar{y}_2)^2 $$\n",
    "\n",
    "where $\\bar{y}_i$ is the average on the left and the right of the split, and $y_i$ are the observed values of the outcome. \n",
    "\n",
    "This is done by searching over every value of every feature in the group to decide which split gives the greatest reduction in error. The fitting is performed recursively until a stopping criteria is reached.\n",
    "\n",
    "Once the trees are grown, predictions can be made. The new observation instance is scored on every tree in the forest- values are fed through the tree, and every node serves as an if/else statement to decide if the observation should go left or right. When it hits a terminal node (a 'leaf'), the average value of the training data in that node is taken as the tree's prediction. This is then averaged over the results of all the trees in the forest to produce a final prediction.\n",
    "\n",
    "The trees are decorrelated from one another by training them on randomly sampled (bootstrapped) subsets of the training data, and at each split the set of candidate training features are also randomly selected (m are randomly chosen from n features).\n",
    "\n",
    "This model is very powerful, and can produce very high quality predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our data with a train/test split\n",
    "\n",
    "One major danger when building models is to overfit to the training data. This means you have trained your model so specifically to the training data, that it will be describing the noise in the training data as well as any general behaviour. A model which is overfit will perform very well on the training data, but will generalise poorly.\n",
    "\n",
    "To remedy this, we will split our data into an 75:25 split. We will build our model using 75% of the data, and evaluate it on the hold-out 25% of the data. This will give us an indication of the model's true performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a problem...\n",
    "\n",
    "We want to model mixture proportions, and we currently have the ingredients by weight. We also have age in days, which is not a mixture ingredient.\n",
    "\n",
    "Let's normalise our data, so we have ingredients by proportion instead. Pandas comes to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixture_ingredients = [\"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\",\n",
    "                       \"Water\", \"Superplasticizer\", \"CoarseAggregate\",\n",
    "                       \"FineAggregate\"]\n",
    "\n",
    "\n",
    "# we divide every ingredient weight by the sum of each mixture\n",
    "normalised_ingredients = (\n",
    "    concrete\n",
    "        .loc[:, mixture_ingredients]\n",
    "        .div(concrete.sum(axis=1), axis = 0)\n",
    ")\n",
    "\n",
    "\n",
    "# join normalised mixture proportion to age and compressive strength\n",
    "concrete_normalised = pd.merge(\n",
    "    concrete.loc[:, [\"Age\", \"CompressiveStrength\"]],\n",
    "    normalised_ingredients,\n",
    "    left_index = True,\n",
    "    right_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you will not get the same results if you change the seed here.\n",
    "# for reproducibility and consistency, you should always seed any function\n",
    "# that uses a random process\n",
    "train, test = train_test_split(concrete_normalised, train_size = 0.75, test_size = 0.25, random_state = 3456)\n",
    "\n",
    "print(\"train size: {} observations \\ntest size: {} observations\".format(train.shape[0], test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "The Scikit-Learn library has a clean, consistent API- check out the docs [here](scikit-learn.org)\n",
    "\n",
    "It contains an implementation for a random forest regression model, which we can easily fit  using the default parameters.\n",
    "\n",
    "Most machine learning models have hyperparameters, that is, free parameters than need tuning to find the best possible model. We can start by fitting the default: a forest of 10 trees, which some may consider to be more of a shrubbery than a forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only set random state, to ensure reproducible results\n",
    "rf_model = RandomForestRegressor(random_state = 1234)\n",
    "\n",
    "# whar are the default options?\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets fit our data to the default model\n",
    "feature_names = concrete.columns.values.tolist()\n",
    "outcome_name = \"CompressiveStrength\"\n",
    "feature_names.remove(outcome_name)\n",
    "\n",
    "rf_model.fit(train.loc[:, feature_names], train.loc[:, outcome_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well do we perform on our training data?\n",
    "\n",
    "Now our model is fit to the trainig data, we can investigate its performance by plotting the predictions of compressive strength vs the actual values. We can also investigate the distribution of the errors.\n",
    "\n",
    "Also, we can calculate the root mean square error, which will give us the average absolute error of our predictions in MPa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_train = rf_model.predict(train.loc[:, feature_names])\n",
    "error_train = predictions_train - train.loc[:, outcome_name].values\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, figsize = (12, 6), dpi=80)\n",
    "\n",
    "for i in range(2):\n",
    "    axarr[i].spines['right'].set_visible(False)\n",
    "    axarr[i].spines['top'].set_visible(False)\n",
    "\n",
    "    \n",
    "axarr[0].scatter(predictions_train, train.loc[:, outcome_name].values, alpha = 0.6)\n",
    "axarr[0].set_xlabel('prediction train')\n",
    "axarr[0].set_ylabel('outcome train')\n",
    "axarr[0].set_xlim(0, 90)\n",
    "axarr[0].set_ylim(0, 90)\n",
    "\n",
    "axarr[1].scatter(train.loc[:, outcome_name].values, error_train, alpha = 0.6)\n",
    "axarr[1].set_xlabel('compressive strength')\n",
    "axarr[1].set_ylabel('prediction train - outcome train')\n",
    "\n",
    "f.subplots_adjust(wspace = 0.3);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what is the rmse on the training set?\n",
    "print(\"training set rmse: {}\".format(mean_squared_error( train.loc[:, outcome_name].values, predictions_train)** 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to mention we see an effect called heterosketadicity here- our model's performance quality is not consistent for higher compressive strengths. This isnt suprising, as we have fewer observations for compressive strength > 60, however it is not ideal as we want to build the strongest mixture. This is something we would come back to try and remedy if we were performing a serious study. \n",
    "\n",
    "For instance, we could redefine the cost function to more harshly penalise errors when the true value of the mixture is > 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things are too good to be true...\n",
    "\n",
    "If we reported these results, we are making the most basic mistake! We have likely overfit to our training data to a degree, and we should evaluate the model on our independent test data set to get an idea of how well the model generalises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_test = rf_model.predict(test.loc[:, feature_names])\n",
    "error_test = predictions_test - test.loc[:, outcome_name].values\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, figsize = (12, 6), dpi=80)\n",
    "\n",
    "for i in range(2):\n",
    "    axarr[i].spines['right'].set_visible(False)\n",
    "    axarr[i].spines['top'].set_visible(False)\n",
    "\n",
    "    \n",
    "axarr[0].scatter(predictions_test, test.loc[:, outcome_name].values, alpha = 0.6)\n",
    "axarr[0].set_xlabel('prediction test')\n",
    "axarr[0].set_ylabel('outcome test')\n",
    "axarr[0].set_xlim(0, 90)\n",
    "axarr[0].set_ylim(0, 90)\n",
    "\n",
    "axarr[1].scatter(test.loc[:, outcome_name].values, error_test, alpha = 0.6)\n",
    "axarr[1].set_xlabel('observation number')\n",
    "axarr[1].set_ylabel('prediction test - outcome test')\n",
    "\n",
    "f.subplots_adjust(wspace = 0.3);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"test set rmse: {}\".format(mean_squared_error( test.loc[:, outcome_name].values, predictions_test)** 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set root mean square error is higher- essentially this means on average, our predictions are $\\pm$ 5MPa.\n",
    "Seeing a higher RMSE for the test set isn't unexpected, as the predictions on the training data have overfit to describe the training data too specifically, and will never perform as well on unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: hyperparameter tuning\n",
    "\n",
    "There is never any guarentee that the default hyperparameters are the best choice for the model. Your task is to tune the number of trees, depth of trees, and number of features to randomly select in the model.\n",
    "\n",
    "I have provided a function which will allow you to test altering the hyperparameters, showing the resulting train and test error.\n",
    "\n",
    "Your aim is to minimise the RMSE on the test data by experimenting values for n_trees, n_features and max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell, butdont touch any of the code!\n",
    "def train_test(features_train, outcome_train, \n",
    "               features_test, outcome_test,\n",
    "               n_trees = 100,\n",
    "               n_features = 3,\n",
    "               max_depth = 6):\n",
    "    model  = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = max_depth,\n",
    "                                   max_features = n_features,\n",
    "                                   random_state = 1234)\n",
    "    \n",
    "    model.fit(features_train, outcome_train)\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "    \n",
    "    rmse_train = mean_squared_error(outcome_train, pred_train) ** 0.5\n",
    "    rmse_test = mean_squared_error(outcome_test, pred_test) ** 0.5\n",
    "\n",
    "    print(\"rmse train: {}\".format(rmse_train))\n",
    "    print(\"rmse test: {}\".format(rmse_test))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# investigate how you can alter n_trees, n_featues and max_depth to beat this default model\n",
    "# note that there are are only 8 features so n_features cannot exceed this\n",
    "train_test(train.loc[:, feature_names], train.loc[:, outcome_name], # do not touch\n",
    "           test.loc[:, feature_names], test.loc[:, outcome_name], # do not touch\n",
    "           n_trees = 10, \n",
    "           n_features = 4,\n",
    "           max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Now, this is a naive way to tune the model hyperparameters. Cross validation is often used for this purpose. There are several variations, but k-fold cross validation is most common, often used in conjunction with a grid search.\n",
    "\n",
    "The idea is to define a number of combinations of the values of the hyperparameters, which is the grid of values we want to search over. For each combinations of hyperparameter, we split the data into k folds, where we train on $k-1$ and test on the left out fold. We then iterate through all the folds, building and evaluating the model on each combination of the training data. \n",
    "\n",
    "This allows us to select the 'best' model on training data, and then we can apply it to the test data set for a final validation. The model would then be ready to apply to any new data samples.\n",
    "\n",
    "For time considerations we will not investigate now, but if anyone is interested, do investigate the GridSearchCV class in the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As well as the RF model, we will also train a GBR model\n",
    "\n",
    "Think of this as a complex 'black box' model. It can produce very high quality predictions when tuned properly. We dont have time to discuss now, but if you are interested to understand how boosting works, the scikit-learn documentation is a good place to start. 'AdaBoost' is probably the most intuitive boosting scheme. We focussed on the random forest model previously as it is easier to understand intuitively.\n",
    "\n",
    "The GBR model also makes high quality predictions when tuned well, and can also be much much faster to make predictions once trained than a random forest. This is important for the mixture optimisation we will perform in the next part of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators = 1000, max_depth = 2, random_state = 1234)\n",
    "gbr.fit(train.loc[:, feature_names], train.loc[:, outcome_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Optimising mixtures\n",
    "\n",
    "Our aim was to be able to predict mixtures that would lead to optimal compressive strengths to guide experminentation. We now have a model that can predict compressive strength based on mixture proportions, and the final step is to use an optimiser to find the mixture that produces the strongest concrete.\n",
    "\n",
    "We will use the [Scipy library](https://www.scipy.org/), which contains an optimiser for this purpose. We firstly have to define a cost function which we want to minimise, and then we will run the optmiser from different starting points to find our candidate mixtures. Using different starting points is important, as we have no guarentee to find the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will look at 28 day old samples\n",
    " \n",
    "We saw age was a feature, and is not part of a mixture. This is because concrete takes time to properly cure. As we had many observations for 28 day old samples, we will limit our investigation to these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixture_ingredients = ['Cement','BlastFurnaceSlag','FlyAsh','Water',\n",
    "                       'Superplasticizer','CoarseAggregate','FineAggregate','Age']\n",
    "\n",
    "test_age_28 = test.loc[test.Age == 28, mixture_ingredients]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to optimise\n",
    "\n",
    "The optimiser needs a cost ('objective') function to work with. It will take this function and some starting values, and try to find a minimum.\n",
    "\n",
    "The cost function takes in n-1 of our mixture proportions, and infers the last. We do this to ensure our mixture proportions sum exactly to 1. Once that process is complete, we append the age (28) to our feature vector. This is then fed into our model, and a prediction is generated. The optimiser will explore mixtures from a given starting point to try and find an optimal value, i.e. the mixture proportions which yield the greatest compressive strength.\n",
    "\n",
    "As the optimiser is searching for a minimum, we return the negated compressive strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assume numpy array\n",
    "# x[0] = Cement\n",
    "# x[1] = BlastFurnaceSlag\n",
    "# x[2] = FlyAsh\n",
    "# x[3] = Water\n",
    "# x[4] = Superplasticizer\n",
    "# x[5] = CoarseAggregate\n",
    "\n",
    "def cost_function(x, model):\n",
    "    \n",
    "    # Check to make sure the mixture proportions are in the correct range\n",
    "    # 'explode' the cost function if we violate\n",
    "    if(x[0] < 0.0 or x[1] > 1.0): return(10**38)\n",
    "    if(x[1] < 0.0 or x[1] > 1.0): return(10**38)\n",
    "    if(x[2] < 0.0 or x[2] > 1.0): return(10**38)\n",
    "    if(x[3] < 0.0 or x[3] > 1.0): return(10**38)\n",
    "    if(x[4] < 0.0 or x[4] > 1.0): return(10**38)\n",
    "    if(x[5] < 0.0 or x[5] > 1.0): return(10**38)\n",
    "    \n",
    "    # x[6] = FineAggregate, the proportion is 1 - the rest\n",
    "    x = np.append(x, (1 - np.sum(x)))\n",
    "    \n",
    "    # if unrealistic amount of water\n",
    "    if(x[3] < 0.05): return(10**38)\n",
    "\n",
    "    # add age = 28\n",
    "    x = np.append(x, 28.0)\n",
    "    \n",
    "    # return the prediction from the model\n",
    "    # negate as we want the largest compressive strength, \n",
    "    # and the optimiser will minimise\n",
    "    return -model.predict(x[None, :])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture optimisation\n",
    "\n",
    "\n",
    "Our strategy is to use  each observation in test set as a starting point (as we know they are realistic mixtures) and try to optimise.\n",
    "\n",
    "Our cost function is not smooth, so we need to be careful which method we use. Nealder-Mead should perform well here.\n",
    "\n",
    "We see not all starting values lead to a solution. We will run an optimisation for each starting value, and return the mixtures which we predict will give the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.empty((115, 9))\n",
    "cols = ['Cement','BlastFurnaceSlag','FlyAsh','Water','Superplasticizer','CoarseAggregate']\n",
    "index_vals = test_age_28.index.values\n",
    "\n",
    "for i, ix in enumerate(index_vals):\n",
    "    \n",
    "    # current starting values\n",
    "    curr_values = test_age_28.loc[ix, cols].values\n",
    "    \n",
    "    # perform the optimization\n",
    "    opt = minimize(cost_function, curr_values, args=(gbr), method = \"Nelder-Mead\", options={'maxiter': 5000})\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print(\"===============\")\n",
    "        print(\"starting sample\", i)\n",
    "        print(\"compressive strength:\",  -opt.fun)\n",
    "        print(\"mixture:\", opt.x)\n",
    "        print()\n",
    "    \n",
    "    # store our mixtures and prediction of compressive strength\n",
    "    res[i,:6] = opt.x\n",
    "    res[i, 6] = (1.0 - opt.x.sum())\n",
    "    res[i, 7] = 28.0\n",
    "    res[i, 8] = -opt.fun\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets pull out our best predictions\n",
    "(\n",
    "    pd.DataFrame(res, columns = mixture_ingredients + ['CompressiveStrength'])\n",
    "        .sort_values(by = 'CompressiveStrength', ascending = False)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have achieved our objective: we have produced a selection of mixes we can propose for further experimentation.\n",
    "\n",
    "To expand this work, we could include other consideration, for instance financial cost. The cost function could be enhanced with desirability function. For instance, if cement is very expensive, the fourth strongest mix prediction would be prohibitave.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python datSci3",
   "language": "python",
   "name": "datsci3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
